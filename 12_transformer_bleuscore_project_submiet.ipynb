{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intensive-floor",
   "metadata": {},
   "source": [
    "# 0. 환경조성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-intranet",
   "metadata": {},
   "source": [
    "## (1) 라이브러리와 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab 설치\n",
    "$ sudo apt-get install curl git\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opposite-conference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import gensim   # 버전 4.x\n",
    "from gensim.models import Word2Vec\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-tutorial",
   "metadata": {},
   "source": [
    "# Step1. 데이터 다운로드\n",
    "- songys/Chatbot_data  https://github.com/songys/Chatbot_data\n",
    "- ChatbotData.csv 를 다운로드해 챗봇 훈련 데이터를 확보\n",
    "- pandas 라이브러리로 .csv파일 읽음\n",
    "- 읽어 온 데이터의 질문과 답변을 각각 questions, anwers 변수에 나눠서 저장\n",
    "- 심볼릭 링크 생성하면 데이터를 다운로드 할 필요가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-scotland",
   "metadata": {},
   "source": [
    "$wget https://github.com/songys/Chatbot_data\n",
    "\n",
    "$mv ChatbotData\\ .csv ~/aiffel/transformer_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ultimate-doctrine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = os.getenv('HOME') + '/aiffel/NLP/12_transformer_chatbot/ChatbotData .csv'\n",
    "data = pd.read_csv(path_to_file, encoding='UTF-8')\n",
    "data\n",
    "\n",
    "#for sen in corpus[0:100][::20]: print('>>', sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q데이터 A데이터 저장하기\n",
    "src = []\n",
    "tgt = []\n",
    "for s,t in zip(data['Q'],data['A']):\n",
    "    src.append(str(s))\n",
    "    tgt.append(str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "remarkable-voice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinct-corner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12시 땡!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corporate-tunnel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controversial-medium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루가 또 가네요.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "portable-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-logan",
   "metadata": {},
   "source": [
    "# Step2. 데이터 정제\n",
    "\n",
    "### preprocess_sentence() 함수\n",
    "1. 영문자의 경우, 모두 소문자로 변환한다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "concerned-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Mecab\n",
    "# mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    from konlpy.tag import Mecab\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    corpus = mecab.morphs(sentence)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bizarre-application",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['소스', '문장', '데이터', '와', '타', '겟', '문장', '데이터', '를', '입력', '으로', '받', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence('소스 문장 데이터와 타겟 문장 데이터를 입력으로 받습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "double-bernard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '땡', '!']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(src[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "civil-automation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하루', '가', '또', '가', '네요', '.']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(tgt[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-connecticut",
   "metadata": {},
   "source": [
    "# Step3. 데이터 토큰화\n",
    "\n",
    "- 토큰화에는 KoNLPy의 mecab 클래스를 사용한다\n",
    "\n",
    "## build_corpus함수 생성\n",
    "1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받는다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화한다.\n",
    "3. 토큰화는 전달받은 토큰나이즈 함수를 사용한다.\n",
    "   - mecab.morphs 함수를 전달\n",
    "4. 토큰의 갯수가 일정 길이 이상인 문장은 데이터에서 제외한다.\n",
    "5. 중복되는 문장은 데이터에서 제외한다.\n",
    "   - 소스:타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사\n",
    "     - 중복쌍이 흐트러지지 않도록 유의\n",
    "     \n",
    "\n",
    "- 구현한 함수를 활용하여 questions와 answers를 각각 que_corpus, ans_corpus에 토큰화하여 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "living-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(src,tgt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "joint-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11750\n",
      "11750\n",
      "Questions ['정성', '담긴', '선물', '뭐', '가', '좋', '을까', '?']\n",
      "Answers: ['손', '편지', '도', '좋', '을', '거', '같', '아요', '.']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정제\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "\n",
    "for tmp in cleaned_corpus:\n",
    "    #print(tmp[0])\n",
    "    #print(tmp[1])\n",
    "    tmp_src = preprocess_sentence(tmp[0])\n",
    "    tmp_tgt = preprocess_sentence(tmp[1])\n",
    "    #if len(tmp_ko) <= 40:\n",
    "    src_corpus.append(tmp_src)\n",
    "    tgt_corpus.append(tmp_tgt)\n",
    "\n",
    "    \n",
    "\n",
    "print(len(src_corpus))\n",
    "print(len(tgt_corpus))\n",
    "print(\"Questions\", src_corpus[100])   \n",
    "print(\"Answers:\", tgt_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedicated-scene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['헤어진', '여자', '친구', '와', '재회', '하', '고', '싶', '어'],\n",
       " ['내일', '뭐', '입', '고', '나갈까'],\n",
       " ['쉬운', '게', '하나', '도', '없', '는', '듯'],\n",
       " ['내', '사랑', '이', '있', '을까', '?'],\n",
       " ['내', '가', '생각', '했', '던', '사람', '이', '맞', '는지', '확신', '이', '안', '들', '어'],\n",
       " ['다른', '사람', '들', '도', '이런', '재회', '바라', '나', '?'],\n",
       " ['썸', '타', '는', '중', '인데', '다른', '사람', '과', '연락', '자제', '해야', '해', '?'],\n",
       " ['드디어', '그', '순간', '이', '거의', '다가왔', '습니다', '.'],\n",
       " ['다가오', '는', '크리', '마스'],\n",
       " ['환기', '좀', '해야', '할까', '?']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "white-cooler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['후회', '하', '지', '않', '을', '자신', '이', '있', '으면', '연락', '해', '보', '세요', '.'],\n",
       " ['날씨', '에', '맞', '게', '입', '는', '게', '좋', '을', '것', '같', '아요', '.'],\n",
       " ['저', '도', '어려운', '게', '투성이', '에요', '.'],\n",
       " ['눈', '을', '크', '게', '뜨', '고', '잘', '찾아보', '세요', '.'],\n",
       " ['새로운', '모습', '을', '봤', '나', '봐요', '.'],\n",
       " ['재회', '는', '한', '번', '쯤', '꿈', '꿎'],\n",
       " ['잘',\n",
       "  '되',\n",
       "  '고',\n",
       "  '싶',\n",
       "  '다면',\n",
       "  '자제',\n",
       "  '하',\n",
       "  '는',\n",
       "  '것',\n",
       "  '도',\n",
       "  '한',\n",
       "  '방법',\n",
       "  '이',\n",
       "  '겠',\n",
       "  '죠',\n",
       "  '.'],\n",
       " ['마지막', '이', '온', '걸까요', '.'],\n",
       " ['빨간', '날', '일', '뿐', '이', '죠', '.'],\n",
       " ['매일', '환기', '하', '는', '게', '좋', '대요', '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "grave-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus = src_corpus\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-mercy",
   "metadata": {},
   "source": [
    "# Step4. Augmentation\n",
    "- 1만 개 가량 데이터 => Lexical Substitution 함수를 적용하여 데이터 확대\n",
    "- 한국어 事前 훈련된 Embedding 모델을 다운로드\n",
    "   - Kyubyong/wordvectors  https://github.com/Kyubyong/wordvectors\n",
    "     - Korean(w) : Word2Vec으로 학습한 모델\n",
    "     - Korean(w)를 다운로드하고, ko.bin 파일을 확보 \n",
    "- Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "military-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim                        4.0.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "vanilla-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#word2vec_path = os.getenv('HOME') + '/aiffel/NLP/12_transformer_chatbot/ko.bin'\n",
    "#word2vec = Word2Vec.load(word2vec_path)  \n",
    "word2vec_path = os.getenv('HOME') + '/aiffel/NLP/12_transformer_chatbot/wiki.ko.vec'\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "worse-portland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜증날', 0.8327823281288147),\n",
       " ('짜증나', 0.8251842260360718),\n",
       " ('짜증나는', 0.8198283314704895),\n",
       " ('짜증나고', 0.806260347366333),\n",
       " ('짜증난다', 0.795647144317627),\n",
       " ('짜증내는', 0.788124680519104),\n",
       " ('짜증나서', 0.784796953201294),\n",
       " ('짜증이', 0.7729315161705017),\n",
       " ('짜증나게', 0.7662945985794067),\n",
       " ('싫증', 0.7639327645301819)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"짜증\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sophisticated-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('분노나', 0.7223528027534485),\n",
       " ('분노도', 0.6828715801239014),\n",
       " ('분노에', 0.6590042114257812),\n",
       " ('분노와', 0.6552071571350098),\n",
       " ('분노감을', 0.6441459059715271),\n",
       " ('분노로', 0.6439695358276367),\n",
       " ('분노는', 0.6429007053375244),\n",
       " ('분노할', 0.641103208065033),\n",
       " ('분노염을', 0.6329343318939209),\n",
       " ('분노하는', 0.6321898698806763)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"분노\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-narrative",
   "metadata": {},
   "source": [
    "## Lexical Substitution 구현하기\n",
    "- 입력된 문장을 Embedding 유사도를 기반으로 Augmentation하여 반환하는 lexical_sub()을 구현\n",
    "- 구현한 함수를 활용해 3,000개의 영문 데이터를 Augmentation하고 결과를 확인\n",
    "- 단어장에 포함되지 않은 단어가 들어오는 경우, 문장 부호에 대한 치환이 발생하는 경우 등의 예외는 자유롭게 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "foreign-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어|\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "swiss-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['정성', '담긴', '선물', '뭐', '가', '좋', '을까', '?']\n",
      "내일쯤 뭐 입 고 나갈까 \n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[100])\n",
    "print(lexical_sub(que_corpus[1], word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fewer-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6df26b2a38e46508a2b63ea357f3582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed54b355884846a24439beb1431fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "# Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], word2vec)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], word2vec)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "       \n",
    "    else:\n",
    "       \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "extensive-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['헤어진', '여자', '친구', '와', '재회', '하', '고', '싶', '어어'], ['내일', '뭐', '입과', '고', '나갈까'], ['쉬운', '것도', '하나', '도', '없', '는', '듯'], ['내', '사랑', '그', '있', '을까', '?'], ['다른', '사람', '들', '도', '이런', '재회', '바라', '와', '?'], ['썸', '타', '는', '중', '인데', '다른', '사람', '와', '연락', '자제', '해야', '해', '?'], ['드디어', '이', '순간', '이', '거의', '다가왔', '습니다', '.'], ['다가오', '는', '크리', '테라포마스'], ['환기', '좀', '해야', '할까', '나요'], ['길', '이', '안되도', '보여']]\n",
      "[['후회', '하', '지', '않', '을', '자신', '이', '있', '으면', '연락', '해', '보', '세요', '.'], ['날씨', '에', '맞', '게', '입', '는', '게', '좋', '을', '것', '같', '아요', '.'], ['저', '도', '어려운', '게', '투성이', '에요', '.'], ['눈', '을', '크', '게', '뜨', '고', '잘', '찾아보', '세요', '.'], ['재회', '는', '한', '번', '쯤', '꿈', '꿎'], ['잘', '되', '고', '싶', '다면', '자제', '하', '는', '것', '도', '한', '방법', '이', '겠', '죠', '.'], ['마지막', '이', '온', '걸까요', '.'], ['빨간', '날', '일', '뿐', '이', '죠', '.'], ['매일', '환기', '하', '는', '게', '좋', '대요', '.'], ['너무', '낙담', '하', '지', '마세요', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(new_que_corpus[:10])\n",
    "print(new_ans_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "correct-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21798\n",
      "21798\n"
     ]
    }
   ],
   "source": [
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "editorial-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33548\n",
      "33548\n"
     ]
    }
   ],
   "source": [
    "que_corpus = que_corpus + new_que_corpus\n",
    "print(len(que_corpus))\n",
    "ans_corpus = ans_corpus + new_ans_corpus\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "comfortable-collaboration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['헤어진지', '두', '달', '늘', '만났', '던', '장소는', '를', '.', '난', '지나다니', '고']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[20400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "minimal-philippines",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['무덤덤', '해', '지', '길', '바랄게요', '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[20400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-seeking",
   "metadata": {},
   "source": [
    "# Step5. 데이터 벡터화\n",
    "- 타겟 데이터인 ans_corpus 에 \\<start> 토큰과 \\<end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. \n",
    "- 우리가 구축한 ans_corpus 는 list 형태임을 활용\n",
    "\n",
    "1. 타겟 데이터 전체에 \\<start>와 \\<end>토큰을 추가한다.\n",
    "   - 챗봇 훈련 데이터의 가장 큰 특징 중 하나는 소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것임\n",
    "   - Embedding층을 공유할 수 있음\n",
    "   \n",
    "2. 특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus와 결합하여 전체 데이터에 대한 단어 사전을 구축하고 벡터화하여 enc_train과 dec_train을 확보한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "raising-choice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '후회', '하', '지', '않', '을', '자신', '이', '있', '으면', '연락', '해', '보', '세요', '.', '<end>']\n",
      "['<start>', '실수', '할', '수', '도', '있', '지요', '.', '<end>']\n",
      "['<start>', '그분', '의', '관심사', '에', '대한', '얘기', '가', '좋', '겠', '네요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "tgt_corpus = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    tgt_corpus.append([\"<start>\"] + corpus + [\"<end>\"])\n",
    "    \n",
    "print(tgt_corpus[0])\n",
    "print(tgt_corpus[325])\n",
    "print(tgt_corpus[395])\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "welcome-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "voc_data = que_corpus + ans_corpus\n",
    "\n",
    "words = np.concatenate(voc_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sublime-hundred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33548\n",
      "33548\n"
     ]
    }
   ],
   "source": [
    "# # 벡터화\n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data\n",
    "\n",
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)\n",
    "\n",
    "print(len(que_train))\n",
    "print(len(ans_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dutch-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33212\n",
      "336\n",
      "33212\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "# # 패팅처리\n",
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(que_train, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans_train, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01) # test set은 1%만\n",
    "\n",
    "print(len(enc_train))\n",
    "print(len(enc_val)) \n",
    "print(len(dec_train))\n",
    "print(len(dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "distributed-minnesota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 646, 1807,  157,  724,   20,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "emerging-burning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3, 856, 814, 174, 157, 574,  10,   2,   4,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "collect-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_train[0]))\n",
    "print(len(dec_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-message",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_tokenizer(corpus,\n",
    "#                       vocab_size,\n",
    "#                       lang='spa-eng',\n",
    "#                       pad_id=0,    # pad token의 일련번호\n",
    "#                       bos_id=1,    # 문장의 시작을 의미하는 bos token\n",
    "#                       eos_id=2,    # 문장의 끝을 의미하는 eos token\n",
    "#                       unk_id=3):   # 모르는 단어 unk token\n",
    "#     file = f'./{lang}_corpus.txt' \n",
    "#     model = f'{lang}_spm'\n",
    "    \n",
    "#     with open(file, 'w', encoding='UTF-8') as f:\n",
    "#         for row in corpus: f.write(str(row) + '\\n')\n",
    "            \n",
    "#     import sentencepiece as spm\n",
    "#     spm.SentencePieceTrainer.Train(\n",
    "#         f'--input=./{file} --model_prefix={model} --vocab_size={vocab_size}' + \n",
    "#         f'--pad_id=={pad_id} --bos_id={bos_id} --eos_id={eos_id} --unk_id={unk_id}'\n",
    "#     )\n",
    "    \n",
    "#     tokenizer = spm.SentencePieceProcessor()\n",
    "#     tokenizer.Load(f'{model}.model')\n",
    "    \n",
    "#     return tokenizer\n",
    "# print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중복 단어 제거\n",
    "# src_corpus = [tuple(l) for l in src_corpus]  # 리스트의 리스트로 튜플로 변경후 set으로 변환\n",
    "# src_cleaned_corpus = set(src_corpus)\n",
    "# src_cleaned_corpus = list(src_cleaned_corpus)\n",
    "\n",
    "\n",
    "# VOCAB_SIZE = 2245\n",
    "# tokenizer = generate_tokenizer(src_cleaned_corpus, VOCAB_SIZE)\n",
    "# tokenizer.set_encode_extra_options('bos:eos') # 문장 양 끝에 <s>, </s>추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-anatomy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 패팅처리\n",
    "# enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "# dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "# enc_train, enc_val, dec_train, dec_val = train_test_split(enc_tensor, dec_tensor,\n",
    "#                                                          test_size=0.01)\n",
    "\n",
    "# print(f'enc_train : {len(enc_train)}', f'    enc_val : {len(enc_val)}')\n",
    "# print(f'dec_train : {len(dec_train)}', f'    dec_val : {len(dec_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-klein",
   "metadata": {},
   "source": [
    "# Step6. 훈련하기\n",
    "- 앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용한다.\n",
    "- 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. \n",
    "- 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! 가장 멋진 답변과 모델의 하이퍼파라미터를 제출하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-operation",
   "metadata": {},
   "source": [
    "\\# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "\\# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-kuwait",
   "metadata": {},
   "source": [
    "## (1) 트랜스포머 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-insertion",
   "metadata": {},
   "source": [
    "### 1) Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "assigned-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    # 배열의 짝수 인덱스에 사인(sin)을 적용합니다: 2i\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에 코사인(cos)을 적용합니다: 2i+1\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-gender",
   "metadata": {},
   "source": [
    "### 2) 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "stuck-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # 패딩을 넣기 위해 어텐션 로짓(logit)에 추가적인 차원을 넣습니다.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]   # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "# 향후 토큰을 순차적으로 마스킹하는 데 사용됩니다. \n",
    "# 즉, 마스크는 사용할 수 없는 항목을 나타냅니다.\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    # np.cumsum(): 배열에서 행에 따라 누적되는 원소들의 누적합 계산\n",
    "    # np.eye(): 대각선이 1인 seq_len x seq_len 크기의 대각행렬 생성\n",
    "    return tf.cast(mask, tf.float32)   # tf.cast: mask(텐서)를 float32로 변환\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "    \n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "    \n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "    \n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "committed-bobby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "generate_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "returning-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.12119007 0.00091314 0.81846964]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[0. 1. 1.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = generate_causality_mask(x.shape[0], x.shape[1])\n",
    "print(x)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-casino",
   "metadata": {},
   "source": [
    "### 3) Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "reserved-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        \n",
    "    def scaled_dot_product_attention(self,Q,K,V,mask):\n",
    "        ''' 어텐션 가중치 계산 '''\n",
    "        \n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "        \n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)   # QK를 스케일링\n",
    "        \n",
    "        # 스케일링된 텐서에 마스크를 더한다.\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        # 소프트맥스의 마지막 축을 정규화하여 스코어의 합이 1이 되도록 만듬\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "        out = tf.matmul(attentions, V)    # 어텐션 가중치와 V(값) 벡터의 곱셈\n",
    "                                    # 포커스하고자 하는 단어가 그대로 유지되고 \n",
    "                                    # 관련 없는 단어가 지워짐\n",
    "        \n",
    "        return out, attentions\n",
    "    \n",
    "    \n",
    "    def split_heads(self,x):\n",
    "        bsz = x.shape[0]  # batch size\n",
    "        # reshape - shape의 한 원소만 -1, \n",
    "        # 의미는 전체 크기가 일정하게 유지되도록 해당 차원의 길이가 자동으로 계산\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])  #  perm은 치환하는 위치를 알려줌\n",
    "        \n",
    "        return split_x\n",
    "        \n",
    "    def combine_heads(self, x):     \n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        # Linear 레이어 추가 - embedding 매핑\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "lyric-charter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n스케일드 닷-프로덕트 어텐션 테스트\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "스케일드 닷-프로덕트 어텐션 테스트\n",
    "'''\n",
    "\n",
    "# def print_out(q, k, v):\n",
    "#     temp_out, temp_attn = scaled_dot_product_attention(\n",
    "#       q, k, v, None)\n",
    "#     print ('어텐션 가중치:')\n",
    "#     print (temp_attn)\n",
    "#     print ('출력 값:')\n",
    "#     print (temp_out)\n",
    "\n",
    "# ---------------\n",
    "# np.set_printoptions(suppress=True)\n",
    "\n",
    "# temp_k = tf.constant([[10,0,0],\n",
    "#                       [0,10,0],\n",
    "#                       [0,0,10],\n",
    "#                       [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "# temp_v = tf.constant([[   1,0],\n",
    "#                       [  10,0],\n",
    "#                       [ 100,5],\n",
    "#                       [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "# print_out(temp_q, temp_k, temp_v)\n",
    "    \n",
    "#     -----------------------\n",
    "# temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "# print_out(temp_q, temp_k, temp_v)\n",
    "\n",
    "# --------------\n",
    "# temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "# print_out(temp_q, temp_k, temp_v)\n",
    "# ---------------------\n",
    "# temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "# print_out(temp_q, temp_k, temp_v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-interim",
   "metadata": {},
   "source": [
    "### 4) Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ethical-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-archives",
   "metadata": {},
   "source": [
    "### 5) Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "altered-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,n_heads,d_ff,dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self,x,mask):\n",
    "        \n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-chase",
   "metadata": {},
   "source": [
    "### 6) Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "executive-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self,x,enc_out,causality_mask,padding_mask):\n",
    "        \n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-society",
   "metadata": {},
   "source": [
    "### 7) Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "scientific-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                n_layers,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                          for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self,x,mask):\n",
    "        out = x\n",
    "        \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-mandate",
   "metadata": {},
   "source": [
    "### 8) Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "linear-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                n_layers,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "                          for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self,x,enc_out,causality_mask,padding_mask):\n",
    "        out = x\n",
    "        \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out,\n",
    "                                                            causality_mask,\n",
    "                                                            padding_mask)\n",
    "            \n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "            \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-wrapping",
   "metadata": {},
   "source": [
    "### 9) Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "becoming-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                n_layers,\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                src_vocab_size,\n",
    "                tgt_vocab_size,\n",
    "                pos_len,\n",
    "                dropout=0.2,\n",
    "                shared_fc=True,\n",
    "                shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "            \n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        self.shared_fc = shared_fc\n",
    "        \n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "            \n",
    "    def embedding(self,emb,x):\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "            \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self,enc_in,dec_in,enc_mask,causality_mask,dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        \n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out,\n",
    "                                                        causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-pennsylvania",
   "metadata": {},
   "source": [
    "### 10) 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "attached-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "d_model = 368\n",
    "\n",
    "transformer = Transformer(n_layers=1,\n",
    "                         d_model=d_model,\n",
    "                         n_heads=8,\n",
    "                         d_ff=1024,\n",
    "                         src_vocab_size=VOCAB_SIZE,\n",
    "                         tgt_vocab_size=VOCAB_SIZE,\n",
    "                         pos_len=200,\n",
    "                         dropout=0.2,\n",
    "                         shared_fc=True,\n",
    "                         shared_emb=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-category",
   "metadata": {},
   "source": [
    "### 11) Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "foreign-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-elite",
   "metadata": {},
   "source": [
    "### 12) Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "atlantic-identification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                    beta_1=0.9,\n",
    "                                    beta_2=0.98,\n",
    "                                    epsilon=1e-9)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-jungle",
   "metadata": {},
   "source": [
    "### 13) Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "final-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-celtic",
   "metadata": {},
   "source": [
    "### 14) Train Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "realistic-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src,tgt,model,optimizer):\n",
    "    tgt_in = tgt[:, :-1]   # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 rihgt shift를 통해  생성한 최종 타겟\n",
    "    \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask,\n",
    "                                                                dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-period",
   "metadata": {},
   "source": [
    "## (2) 훈련을 시키자\n",
    "\n",
    "[예문]   \n",
    "1. 지루하다, 놀러가고 싶어.   \n",
    "2. 오늘 일찍 일어났더니 피곤하다.   \n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.   \n",
    "4. 집에 있는다는 소리야.   \n",
    "\n",
    "---   \n",
    "\n",
    "[제출Translations]   \n",
    "> 1. 잠깐 쉬 어도 돼요 . \\<end>   \n",
    "> 2. 맛난 거 드세요 . \\<end>   \n",
    "> 3. 떨리 겠 죠 . \\<end>   \n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . \\<end>   \n",
    "    \n",
    "[Hyperparameters]    \n",
    "> n_layers: 1   \n",
    "> d_model: 368    \n",
    "> n_heads: 8    \n",
    "> d_ff: 1024   \n",
    "> dropout: 0.2   \n",
    "   \n",
    "[Training Parameters]   \n",
    "> Warmup Steps: 1000   \n",
    "> Batch Size: 64   \n",
    "> Epoch At: 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fleet-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_sentence(encoded_sentence, idx2word):\n",
    "    return ' '.join(idx2word[index] if index in idx2word else '<UNK>' for index in encoded_sentence[1:]) \n",
    "\n",
    "\n",
    "def get_decoded_sentences(encoded_sentences, idx2word):\n",
    "    return [get_decoded_sentence(encoded_sentence, idx2word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "#     pieces = src_tokenizer.encode_as_pieces(sentence) # 문자열을 token으로 분할\n",
    "#     tokens = src_tokenizer.encode_as_ids(sentence) # 문자열을 숫자로 분할\n",
    "    \n",
    "#     _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "#                                                           maxlen=enc_train.shape[-1],\n",
    "#                                                           padding='post')\n",
    "    \n",
    "#     ids = []\n",
    "#     output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "#     for i in range(dec_train.shape[-1]):\n",
    "#         enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "#         predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output,\n",
    "#                                                                 enc_padding_mask,\n",
    "#                                                                 combined_mask,\n",
    "#                                                                 dec_padding_mask)\n",
    "        \n",
    "#         predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0,-1]).numpy().item()\n",
    "#         # predictions에 소프트맥스 함수를 적용하여 가장 큰 값의 인덱스를 predicted_id로 저장\n",
    "        \n",
    "#         if tgt_tokenizer.eos_id() == predicted_id:\n",
    "#             result = tgt_tokenizer.decode_ids(ids) # 숫자를 문자열로 복원\n",
    "#             return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        \n",
    "#         ids.append(predicted_id)\n",
    "#         output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "        \n",
    "#     result = tgt_tokenizer.decode_ids(ids)\n",
    "#     return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "#     pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model,\n",
    "#                                                                   src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "handmade-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model):\n",
    "    # sentence 전처리(enc_train과 같은 모양으로)\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = sentence\n",
    "    tokens = get_encoded_sentence(pieces, word_to_index)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    \n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0) \n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        \n",
    "        # 예측 단어가 종료 토큰일 경우\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "        ##word_to_index\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "disabled-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "controlling-mother",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9476b508d8945a799120350d9a870cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31efd7a3947a41eaab03c23c55dc8ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d481e915a548afb041c840157263f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a7599f9f264f149b17e9d04253ee2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01be88d9c2334bab99c36bc260792850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2169cb95b884d609a0addd9cb851d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7d373af2bd49e2b7124fe7c34e9661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce5e8b4b0de4e3a9d736f384c336a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e348f2de5843e798a232413562d1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1a829511ac4ef797dea4174aed8eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm  # 버전 5.0 이상\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    # t = tqdm_notebook(idx_list)   \n",
    "    t = tqdm(idx_list)    # 버전 5.0 이상\n",
    "    \n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                                                     dec_train[idx:idx+BATCH_SIZE],\n",
    "                                                                     transformer,\n",
    "                                                                     optimizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str(f'Epoch {epoch + 1}')\n",
    "        t.set_postfix_str(f'Loss{total_loss.numpy() / (batch + 1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cutting-scroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations\n",
      "Input: 지루하다, 놀러가고 싶어.\n",
      "Predicted translation: 생각 보다 나 봐요 .\n",
      "Input: 오늘 일찍 일어났더니 피곤하다.\n",
      "Predicted translation: 노동 을 했 나 봐요 .\n",
      "Input: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "Predicted translation: 이 많 은 것 같 아요 .\n",
      "Input: 집에 있는다는 소리야.\n",
      "Predicted translation: 은 알 게 된 관계 가 좋 아 질 거 예요 .\n"
     ]
    }
   ],
   "source": [
    "print(\"Translations\")   \n",
    "for example in examples:\n",
    "    translate(example, transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-actress",
   "metadata": {},
   "source": [
    "# Step7. 성능 측정하기(BLEU Score)\n",
    "- 챗봇의 경우, 올바른 대댭을 하는지가 중요한 평가지표다. 이를 위한 BLEU Score를 계산하는 calculate_bleu()함수를 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-franchise",
   "metadata": {},
   "source": [
    "### 1) NLTK를 활용한 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "political-viking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다.']\n",
      "번역문 : ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score : 6.5806869883189804e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = '많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다.'.split()\n",
    "candidate = '적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요'.split()\n",
    "\n",
    "print(f'원문 : {reference}')\n",
    "print(f'번역문 : {candidate}')\n",
    "print(f'BLEU Score : {sentence_bleu([reference], candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-illinois",
   "metadata": {},
   "source": [
    "$(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$\n",
    "\n",
    "BLEU Score의 정의로 돌아가 한번 따져봅시다. BLEU Score가 N-gram으로 점수를 측정한다는 것을 기억하실 거예요. 아래 수식을 기억하시죠? 1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면$(^{1/4})$ BLEU Score가 된답니다. 진정 멋진 번역이라면, 모든 N-gram에 대해서 높은 점수를 얻었을 거예요. 그렇다면 위에서 살펴본 예시에서는 각 N-gram이 점수를 얼마나 얻었는지 확인해보도록 합시다. weights의 디폴트값은 [0.25, 0.25, 0.25, 0.25]로 1-gram부터 4-gram까지의 점수에 가중치를 동일하게 주는 것이지만, 만약 이 값을 [1, 0, 0, 0]으로 바꿔주면 BLEU Score에 1-gram의 점수만 반영하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "naughty-pontiac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram : 0.4166666666666667\n",
      "2-gram : 0.0909090909090909\n",
      "3-gram : 2.2250738585072626e-308\n",
      "4-gram : 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "print(f'1-gram : {sentence_bleu([reference], candidate, weights=[1, 0, 0, 0])}')\n",
    "print(f'2-gram : {sentence_bleu([reference], candidate, weights=[0, 1, 0, 0])}')\n",
    "print(f'3-gram : {sentence_bleu([reference], candidate, weights=[0, 0, 1, 0])}')\n",
    "print(f'4-gram : {sentence_bleu([reference], candidate, weights=[0, 0, 0, 1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-circumstances",
   "metadata": {},
   "source": [
    "- BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있습니다. Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉, 우리의 의도대로 점수가 계산되는 거예요.\n",
    "   \n",
    "- 진실된 BLEU Score를 확인하기 위해 어서 SmoothingFunction() 을 적용해봅시다! 아래 코드에서는 SmoothingFunction().method1을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, nltk에서는 method0부터 method7까지를 이미 제공하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-click",
   "metadata": {},
   "source": [
    "### 2) SmoothingFunction()으로 BLEU Score보정하기\n",
    "- BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용\n",
    "- Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉 우리의 의도대로 점수가 계산되는 거예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "median-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 : 0.4166666666666667\n",
      "BLEU-2 : 0.0909090909090909\n",
      "BLEU-3 : 0.010000000000000004\n",
      "BLEU-4 : 0.011111111111111112\n",
      "\n",
      "BLEU-total : 0.045293761707938834\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference], candidate, weights=weights,\n",
    "                        smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print(f'BLEU-1 : {calculate_bleu(reference, candidate, weights=[1, 0, 0, 0])}')\n",
    "print(f'BLEU-2 : {calculate_bleu(reference, candidate, weights=[0, 1, 0, 0])}')\n",
    "print(f'BLEU-3 : {calculate_bleu(reference, candidate, weights=[0, 0, 1, 0])}')\n",
    "print(f'BLEU-4 : {calculate_bleu(reference, candidate, weights=[0, 0, 0, 1])}')\n",
    "\n",
    "print(f'\\nBLEU-total : {calculate_bleu(reference, candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-beijing",
   "metadata": {},
   "source": [
    "### 3) 트랜스포머 모델의 번역 성능 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-hayes",
   "metadata": {},
   "source": [
    "- 여기서 BLEU-4가 BLEU-3보다 약간이나마 점수가 높은 이유는 한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수를 생각해보면 이해할 수 있습니다. 각 Precision을 N-gram 개수로 나누는 부분에서 차이가 발생하는 것이죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "damaged-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "        \n",
    "        src = []\n",
    "        tgt = []\n",
    "        \n",
    "        for word in src_tokens:\n",
    "            if word !=0 and word !=1 and word !=3 and word !=4:\n",
    "                src.append(word)\n",
    "        \n",
    "        for word in tgt_tokens:\n",
    "            if word != 0 and word != 3 and word !=4:\n",
    "                tgt.append(word)\n",
    "\n",
    "        src_sentence = get_decoded_sentence(src, index_to_word)\n",
    "        tgt_sentence = get_decoded_sentence(tgt, index_to_word)\n",
    "        \n",
    "        \n",
    "        reference = preprocess_sentence(tgt_sentence)\n",
    "        candidate = translate(src_sentence, transformer)\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "demanding-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256b115732f49f5b9d9bb9e07901682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 하 네\n",
      "Predicted translation: 세요 .\n",
      "Source Sentence:  하 네\n",
      "Model Prediction:  세요 .\n",
      "Real:  ['이', '복잡', '한가요', '.']\n",
      "Score: 0.080343\n",
      "\n",
      "Input: 미리 걱정 해\n",
      "Predicted translation: 은 먼저 사랑 을 나가 세요 .\n",
      "Source Sentence:  미리 걱정 해\n",
      "Model Prediction:  은 먼저 사랑 을 나가 세요 .\n",
      "Real:  ['데', '없', '는', '걱정', '일', '수', '도', '있', '어요', '.']\n",
      "Score: 0.011503\n",
      "\n",
      "Input: 가 극 에 달했었 는데\n",
      "Predicted translation: 은 괜찮 아 졌 길 바랄게요 .\n",
      "Source Sentence:  가 극 에 달했었 는데\n",
      "Model Prediction:  은 괜찮 아 졌 길 바랄게요 .\n",
      "Real:  ['은', '좀', '괜찮', '아', '졌', '길', '바랄게요', '.']\n",
      "Score: 0.017201\n",
      "\n",
      "Input: 커피 한잔을\n",
      "Predicted translation: 시간 이 라면 저 도 필요 해요 .\n",
      "Source Sentence:  커피 한잔을\n",
      "Model Prediction:  시간 이 라면 저 도 필요 해요 .\n",
      "Real:  ['면', '좋', '죠', '.']\n",
      "Score: 0.012109\n",
      "\n",
      "Input: 주 에 혼자 여행길 가려 하 는데\n",
      "Predicted translation: 은 생각 이 에요 . 혼자 안 좋 은 것 이 힘들 었 겠 어요 .\n",
      "Source Sentence:  주 에 혼자 여행길 가려 하 는데\n",
      "Model Prediction:  은 생각 이 에요 . 혼자 안 좋 은 것 이 힘들 었 겠 어요 .\n",
      "Real:  ['은', '생각', '이', '에요', '.', '혼자', '하', '는', '여행', '은', '기분', '전환', '에', '도움', '이', '돼요', '.']\n",
      "Score: 0.008388\n",
      "\n",
      "Input: 해도 해도 끝 이 없 어\n",
      "Predicted translation: 이 보이 지 않 겠 네요 .\n",
      "Source Sentence:  해도 해도 끝 이 없 어\n",
      "Model Prediction:  이 보이 지 않 겠 네요 .\n",
      "Real:  ['조금', '씩', '해', '보', '세요', '.']\n",
      "Score: 0.015719\n",
      "\n",
      "Input: 정리 어떤 식 으로 해야 할까\n",
      "Predicted translation: 이 되 었 을 때 가 되 었 을 거 예요 .\n",
      "Source Sentence:  정리 어떤 식 으로 해야 할까\n",
      "Model Prediction:  이 되 었 을 때 가 되 었 을 거 예요 .\n",
      "Real:  ['을', '줄이', '고', '안', '하', '는', '거', '겠', '죠', '.']\n",
      "Score: 0.010414\n",
      "\n",
      "Input: 내 이상이어야 형 이 야\n",
      "Predicted translation: 이상 형 을 주 는 사람 이 있 을 것 같 아요 .\n",
      "Source Sentence:  내 이상이어야 형 이 야\n",
      "Model Prediction:  이상 형 을 주 는 사람 이 있 을 것 같 아요 .\n",
      "Real:  ['형', '을', '만나', '는', '경우', '는', '흔치', '않', '는데', '대단', '하', '네요', '.']\n",
      "Score: 0.009499\n",
      "\n",
      "Input: 할 수 있 는 사람 만나 고 싶 어 .\n",
      "Predicted translation: 사람 말 는 상황 이 있 었 나 봐요 .\n",
      "Source Sentence:  할 수 있 는 사람 만나 고 싶 어 .\n",
      "Model Prediction:  사람 말 는 상황 이 있 었 나 봐요 .\n",
      "Real:  ['사람', '만날', '수', '있', '을', '거', '예요', '.']\n",
      "Score: 0.010331\n",
      "\n",
      "Input: 한 냉면 먹 고 싶 어\n",
      "Predicted translation: 만 해도 군침 이 도 네요 .\n",
      "Source Sentence:  한 냉면 먹 고 싶 어\n",
      "Model Prediction:  만 해도 군침 이 도 네요 .\n",
      "Real:  ['만', '해도', '군침', '이', '도', '네요', '또한']\n",
      "Score: 0.016189\n",
      "\n",
      "Input: 도 안 되 네\n",
      "Predicted translation: 은 힘들 어요 .\n",
      "Source Sentence:  도 안 되 네\n",
      "Model Prediction:  은 힘들 어요 .\n",
      "Real:  ['될', '거', '뭐', '예요', '.']\n",
      "Score: 0.023980\n",
      "\n",
      "Input: 주행 자동차 언제 나오 냐\n",
      "Predicted translation: 에 나올 거 같 아요 .\n",
      "Source Sentence:  주행 자동차 언제 나오 냐\n",
      "Model Prediction:  에 나올 거 같 아요 .\n",
      "Real:  ['미래', '가', '나올', '거', '같', '아요', '.']\n",
      "Score: 0.020448\n",
      "\n",
      "Input: 에 투자 하 는 거 어때 ?\n",
      "Predicted translation: 한 생각 이 네요 .\n",
      "Source Sentence:  에 투자 하 는 거 어때 ?\n",
      "Model Prediction:  한 생각 이 네요 .\n",
      "Real:  ['방면', '에', '대한', '지식', '이', '있', '었', '으면', '좋', '겠', '네요', '.']\n",
      "Score: 0.020469\n",
      "\n",
      "Input: 사람 은 잘 만 반고 는 연애\n",
      "Predicted translation: 하 며 있 어요 .\n",
      "Source Sentence:  사람 은 잘 만 반고 는 연애\n",
      "Model Prediction:  하 며 있 어요 .\n",
      "Real:  ['연애', '도', '연애', '에', '요', '.']\n",
      "Score: 0.025099\n",
      "\n",
      "Input: 쓸쓸 합니다\n",
      "Predicted translation: 인 것 같 네요 .\n",
      "Source Sentence:  쓸쓸 합니다\n",
      "Model Prediction:  인 것 같 네요 .\n",
      "Real:  ['에게', '기대', '세요', '.']\n",
      "Score: 0.021105\n",
      "\n",
      "Input: 어딥니까 당연히 예쁜 애 들 많 겠 지\n",
      "Predicted translation: 될 거 예요 .\n",
      "Source Sentence:  어딥니까 당연히 예쁜 애 들 많 겠 지\n",
      "Model Prediction:  될 거 예요 .\n",
      "Real:  ['을', '준비', '하', '니', '일반인', '보다', '다', '예쁘', '겠', '죠', '.']\n",
      "Score: 0.019090\n",
      "\n",
      "Input: 같 은 짓 을 했 어 .\n",
      "Predicted translation: 할 거 예요 .\n",
      "Source Sentence:  같 은 짓 을 했 어 .\n",
      "Model Prediction:  할 거 예요 .\n",
      "Real:  ['자책', '하', '지', '마요', '또한']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 일근\n",
      "Predicted translation: 주무세요 .\n",
      "Source Sentence:  일근\n",
      "Model Prediction:  주무세요 .\n",
      "Real:  ['집', '에', '가', '서', '쉬', '시', '길', '바랄게요', '.']\n",
      "Score: 0.024762\n",
      "\n",
      "Input: 일 일 팩\n",
      "Predicted translation: 관리 !\n",
      "Source Sentence:  일 일 팩\n",
      "Model Prediction:  관리 !\n",
      "Real:  ['관리', '까지', '반고', '시', '다니', '!']\n",
      "Score: 0.048730\n",
      "\n",
      "Input: 가 있 으면 뭐 가 좋 아 ?\n",
      "Predicted translation: 도 궁금 해요 .\n",
      "Source Sentence:  가 있 으면 뭐 가 좋 아 ?\n",
      "Model Prediction:  도 궁금 해요 .\n",
      "Real:  ['친구', '가', '발생', '된다는', '거', '죠', '.']\n",
      "Score: 0.023980\n",
      "\n",
      "Input: 힘 이 르루아 네\n",
      "Predicted translation: 지 말 고 조금 만 해요 .\n",
      "Source Sentence:  힘 이 르루아 네\n",
      "Model Prediction:  지 말 고 조금 만 해요 .\n",
      "Real:  ['은', '날', '이', '있', '을', '거', '예요', '.']\n",
      "Score: 0.013218\n",
      "\n",
      "Input: 면 예민 함 ?\n",
      "Predicted translation: 돼요 .\n",
      "Source Sentence:  면 예민 함 ?\n",
      "Model Prediction:  돼요 .\n",
      "Real:  ['는', '게', '없', '단', '죠', '.']\n",
      "Score: 0.048730\n",
      "\n",
      "Input: 에서 여자 친구 찾 아 볼까 ?\n",
      "Predicted translation: 충분 한 대화 를 좋 은 게 좋 아 봅니다 .\n",
      "Source Sentence:  에서 여자 친구 찾 아 볼까 ?\n",
      "Model Prediction:  충분 한 대화 를 좋 은 게 좋 아 봅니다 .\n",
      "Real:  ['서', '만났', '든', '그', '후', '가', '중요', '해요', '.']\n",
      "Score: 0.007576\n",
      "\n",
      "Input: 밤 하 고 픈 말 다 쏟 아 내리고 합니다 !\n",
      "Predicted translation: 가 들 어 드릴게요 .\n",
      "Source Sentence:  밤 하 고 픈 말 다 쏟 아 내리고 합니다 !\n",
      "Model Prediction:  가 들 어 드릴게요 .\n",
      "Real:  ['가', '다', '들', '어', '드릴게요', '.']\n",
      "Score: 0.024089\n",
      "\n",
      "Input: 년 반의 만난 여자 와 헤어진지 한 달 되 었 습니다\n",
      "Predicted translation: 됐 이지만 제대로 할 수 있 을 거 예요 .\n",
      "Source Sentence:  년 반의 만난 여자 와 헤어진지 한 달 되 었 습니다\n",
      "Model Prediction:  됐 이지만 제대로 할 수 있 을 거 예요 .\n",
      "Real:  ['겠', '지만', '잘', '이겨낼', '수', '있', '을', '거', '예요', '.']\n",
      "Score: 0.011833\n",
      "\n",
      "Input: 에 들 을 노래 뭐 가 좋 을까\n",
      "Predicted translation: 도 함께 결혼 어도 좋 겠 어요 .\n",
      "Source Sentence:  에 들 을 노래 뭐 가 좋 을까\n",
      "Model Prediction:  도 함께 결혼 어도 좋 겠 어요 .\n",
      "Real:  ['.']\n",
      "Score: 0.010182\n",
      "\n",
      "Input: 섬유유연제 향 좋 은 거 사 야 겠지\n",
      "Predicted translation: 이 많 은 걸 말 해 주 죠 .\n",
      "Source Sentence:  섬유유연제 향 좋 은 거 사 야 겠지\n",
      "Model Prediction:  이 많 은 걸 말 해 주 죠 .\n",
      "Real:  ['이', '많', '은', '걸', '말', '해', '주', '죠', '.']\n",
      "Score: 0.019923\n",
      "\n",
      "Input: 문제 가 많 은 거 같 아\n",
      "Predicted translation: 도 사랑 하 는 사람 이 에요 .\n",
      "Source Sentence:  문제 가 많 은 거 같 아\n",
      "Model Prediction:  도 사랑 하 는 사람 이 에요 .\n",
      "Real:  ['는', '해결', '하', '라고', '있', '는', '거', '죠', '.']\n",
      "Score: 0.014217\n",
      "\n",
      "Input: 하 면서 석박 따 기 어렵 겠 지\n",
      "Predicted translation: 살 는 오래 하 는 것 도 중요 한 거 예요 .\n",
      "Source Sentence:  하 면서 석박 따 기 어렵 겠 지\n",
      "Model Prediction:  살 는 오래 하 는 것 도 중요 한 거 예요 .\n",
      "Real:  ['지', '는', '않', '지', '을', '거', '예요', '.']\n",
      "Score: 0.009562\n",
      "\n",
      "Input: 마음 을 보여 주의 는 게 어려워\n",
      "Predicted translation: 저 랑 놀 는 게 어렵 죠 .\n",
      "Source Sentence:  마음 을 보여 주의 는 게 어려워\n",
      "Model Prediction:  저 랑 놀 는 게 어렵 죠 .\n",
      "Real:  ['가', '부담', '스러워', '하', '지', '않', '게', '나', '를', '사랑', '하', '고', '상대', '를', '사랑', '해', '주', '세요', '.']\n",
      "Score: 0.012127\n",
      "\n",
      "Input: 들 초대 하 려고\n",
      "Predicted translation: 하 고 사랑 하 면 좋 을 거 예요 .\n",
      "Source Sentence:  들 초대 하 려고\n",
      "Model Prediction:  하 고 사랑 하 면 좋 을 거 예요 .\n",
      "Real:  ['은', '생각', '이', '네요', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: 만 판정 받어 았 어\n",
      "Predicted translation: 하 게 먹 어 봐요 .\n",
      "Source Sentence:  만 판정 받어 았 어\n",
      "Model Prediction:  하 게 먹 어 봐요 .\n",
      "Real:  ['하', '게', '운동', '해', '보', '세요', '.']\n",
      "Score: 0.022417\n",
      "\n",
      "Input: 에 일어나 는 거 힘들 다\n",
      "Predicted translation: 만 더 버텨 보 세요 .\n",
      "Source Sentence:  에 일어나 는 거 힘들 다\n",
      "Model Prediction:  만 더 버텨 보 세요 .\n",
      "Real:  ['더', '일찍', '잠', '을', '자', '보', '세요', '.']\n",
      "Score: 0.020448\n",
      "\n",
      "Input: 후 에 뭐 하 지\n",
      "Predicted translation: 이 거기 까지 였 나 봐요 .\n",
      "Source Sentence:  후 에 뭐 하 지\n",
      "Model Prediction:  이 거기 까지 였 나 봐요 .\n",
      "Real:  ['이', '꽤', '기', '니까요', '.']\n",
      "Score: 0.016189\n",
      "\n",
      "Input: 배정 내 가 원 하 는 대로 될까\n",
      "Predicted translation: 는 게 더 좋 은 거 예요 .\n",
      "Source Sentence:  배정 내 가 원 하 는 대로 될까\n",
      "Model Prediction:  는 게 더 좋 은 거 예요 .\n",
      "Real:  ['되', '길', '바랍니다', '.']\n",
      "Score: 0.012301\n",
      "\n",
      "Input: 는 사람 많 아\n",
      "Predicted translation: 사람 은 누구 나 한 사람 이 죠 .\n",
      "Source Sentence:  는 사람 많 아\n",
      "Model Prediction:  사람 은 누구 나 한 사람 이 죠 .\n",
      "Real:  ['죠', '!']\n",
      "Score: 0.009630\n",
      "\n",
      "Input: 분다\n",
      "Predicted translation: 에 부 는 모래바람 에 바람 인지 살펴보 세요 .\n",
      "Source Sentence:  분다\n",
      "Model Prediction:  에 부 는 모래바람 에 바람 인지 살펴보 세요 .\n",
      "Real:  ['에', '부', '는', '바람', '인지', '살펴보', '세요', '또한']\n",
      "Score: 0.009187\n",
      "\n",
      "Input: 는데 가끔 궁금 해져\n",
      "Predicted translation: 만 있 어도 괜찮 인 거 예요 .\n",
      "Source Sentence:  는데 가끔 궁금 해져\n",
      "Model Prediction:  만 있 어도 괜찮 인 거 예요 .\n",
      "Real:  ['수', '있', '어요', '.']\n",
      "Score: 0.012846\n",
      "\n",
      "Input: 별 곧 두 달 .\n",
      "Predicted translation: 때 죠 .\n",
      "Source Sentence:  별 곧 두 달 .\n",
      "Model Prediction:  때 죠 .\n",
      "Real:  ['도', '힘드', '시', '군요', '.']\n",
      "Score: 0.053728\n",
      "\n",
      "Input: 팔 수 있 는 곳 알려 줘 ?\n",
      "Predicted translation: 도 모르 은 사람 이 있 을 거 예요 .\n",
      "Source Sentence:  팔 수 있 는 곳 알려 줘 ?\n",
      "Model Prediction:  도 모르 은 사람 이 있 을 거 예요 .\n",
      "Real:  ['고서점', '에서', '팔', '아', '보', '세요', '.']\n",
      "Score: 0.008687\n",
      "\n",
      "Input: 녀 가 연락 안 되 고 있 는 데 자 나 나요\n",
      "Predicted translation: 살 면 운동 하 고 나가 는 것 같 아요 .\n",
      "Source Sentence:  녀 가 연락 안 되 고 있 는 데 자 나 나요\n",
      "Model Prediction:  살 면 운동 하 고 나가 는 것 같 아요 .\n",
      "Real:  ['고', '있', '을지', '도', '모르', '겠', '어요', '.']\n",
      "Score: 0.009410\n",
      "\n",
      "Input: 날짜 잡 으려고\n",
      "Predicted translation: 하 겠 지만 극복 하 군요 .\n",
      "Source Sentence:  날짜 잡 으려고\n",
      "Model Prediction:  하 겠 지만 극복 하 군요 .\n",
      "Real:  ['드려요', '.']\n",
      "Score: 0.012301\n",
      "\n",
      "Input: 네 . 정말 . 이런 건 줄 몰랐 어\n",
      "Predicted translation: 사랑 사람 인가 봐요 .\n",
      "Source Sentence:  네 . 정말 . 이런 건 줄 몰랐 어\n",
      "Model Prediction:  사랑 사람 인가 봐요 .\n",
      "Real:  ['았', '으면', '시작', '못', '했', '겠', '죠', '또한']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 내 가 해야 해야 것\n",
      "Predicted translation: 마련 이 에요 .\n",
      "Source Sentence:  내 가 해야 해야 것\n",
      "Model Prediction:  마련 이 에요 .\n",
      "Real:  ['때리', '기']\n",
      "Score: 0.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 운 이 안 좋 었 어\n",
      "Predicted translation: 곳은 에서 상담 을 받 아 보 세요 .\n",
      "Source Sentence:  운 이 안 좋 었 어\n",
      "Model Prediction:  곳은 에서 상담 을 받 아 보 세요 .\n",
      "Real:  ['곳', '에', '쓰', '려고', '운', '을', '아껴', '뒀', '나', '봐요', '.']\n",
      "Score: 0.012918\n",
      "\n",
      "Input: 한 지 10 일 차 점점 나아지 며 있 습니다 .\n",
      "Predicted translation: 도 인연 이 라면 자연 스럽 게 기본 이 다를 거 예요 .\n",
      "Source Sentence:  한 지 10 일 차 점점 나아지 며 있 습니다 .\n",
      "Model Prediction:  도 인연 이 라면 자연 스럽 게 기본 이 다를 거 예요 .\n",
      "Real:  ['랑', '대화', '하', '는', '게', '위', '로', '가', '되', '었', '으면', '합니다', '.']\n",
      "Score: 0.006938\n",
      "\n",
      "Input: 인데 그냥 취직 할까 ?\n",
      "Predicted translation: 나 할 수 있 을 거 예요 .\n",
      "Source Sentence:  인데 그냥 취직 할까 ?\n",
      "Model Prediction:  나 할 수 있 을 거 예요 .\n",
      "Real:  ['하', '는', '것', '자체', '가', '중요', '하', '죠', '.']\n",
      "Score: 0.012301\n",
      "\n",
      "Input: 는 그대로 의 나 를 사랑 하 기\n",
      "Predicted translation: 사랑 이 자 면 좋 이 이 이 이 이 이 이 가 사랑 이 더 사랑 이 에요 .\n",
      "Source Sentence:  는 그대로 의 나 를 사랑 하 기\n",
      "Model Prediction:  사랑 이 자 면 좋 이 이 이 이 이 이 이 가 사랑 이 더 사랑 이 에요 .\n",
      "Real:  ['사람', '이', '자', '멋진', '말', '이', '네요', '또한']\n",
      "Score: 0.005641\n",
      "\n",
      "Input: 친구 가 잠 이 너무 많 아 .\n",
      "Predicted translation: 을 미리 걱정인 만들 어 보 세요 .\n",
      "Source Sentence:  친구 가 잠 이 너무 많 아 .\n",
      "Model Prediction:  을 미리 걱정인 만들 어 보 세요 .\n",
      "Real:  ['은', '잠꾸러기', '라던데요', '.']\n",
      "Score: 0.009630\n",
      "\n",
      "Input: 한잔 짠\n",
      "Predicted translation: 기 좋 을 거 예요 .\n",
      "Source Sentence:  한잔 짠\n",
      "Model Prediction:  기 좋 을 거 예요 .\n",
      "Real:  ['좋', '죠', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Input: 너무 탔 다\n",
      "Predicted translation: 쓰 지 마세요 .\n",
      "Source Sentence:  너무 탔 다\n",
      "Model Prediction:  쓰 지 마세요 .\n",
      "Real:  ['해', '주', '세요', '또한']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 을 정도로 보 고 싶 다 .\n",
      "Predicted translation: 좋 은 사람 들 이 있 을 거 예요 .\n",
      "Source Sentence:  을 정도로 보 고 싶 다 .\n",
      "Model Prediction:  좋 은 사람 들 이 있 을 거 예요 .\n",
      "Real:  ['사랑', '했', '나', '봐요', '.']\n",
      "Score: 0.009134\n",
      "\n",
      "Input: 충분히 좋 은 사람 이 였 어\n",
      "Predicted translation: 은 사람 만나 면 좋 아 할 거 예요 .\n",
      "Source Sentence:  충분히 좋 은 사람 이 였 어\n",
      "Model Prediction:  은 사람 만나 면 좋 아 할 거 예요 .\n",
      "Real:  ['도', '충분히', '좋', '은', '사람', '이', '었', '어요', '.']\n",
      "Score: 0.011433\n",
      "\n",
      "Input: 생활 해야 되 는 뭐 싫 다\n",
      "Predicted translation: 이 서 지 않 아요 .\n",
      "Source Sentence:  생활 해야 되 는 뭐 싫 다\n",
      "Model Prediction:  이 서 지 않 아요 .\n",
      "Real:  ['적응', '될', '거', '예요', '.']\n",
      "Score: 0.017033\n",
      "\n",
      "Input: 랑 술 한잔 기울이 고 들어왔 습니다 .\n",
      "Predicted translation: 지 않지 은 결정 이 었 나 봐요 .\n",
      "Source Sentence:  랑 술 한잔 기울이 고 들어왔 습니다 .\n",
      "Model Prediction:  지 않지 은 결정 이 었 나 봐요 .\n",
      "Real:  ['이', '조금', '이나마', '나아졌', '길', '바랍니다', '.']\n",
      "Score: 0.011452\n",
      "\n",
      "Input: 의 자만심 때문 에\n",
      "Predicted translation: 심 다 올 거 예요 .\n",
      "Source Sentence:  의 자만심 때문 에\n",
      "Model Prediction:  심 다 올 거 예요 .\n",
      "Real:  ['심', '을', '가져', '보', '세요', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Input: 의 결과 는 어차피 네 가지\n",
      "Predicted translation: 이 있 을 거 예요 .\n",
      "Source Sentence:  의 결과 는 어차피 네 가지\n",
      "Model Prediction:  이 있 을 거 예요 .\n",
      "Real:  ['이', '더', '중요', '하', '지요', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Input: 한 돌돌 싱 녀\n",
      "Predicted translation: 게 힘들 죠 .\n",
      "Source Sentence:  한 돌돌 싱 녀\n",
      "Model Prediction:  게 힘들 죠 .\n",
      "Real:  ['매일', '행복', '한', '일', '이', '생기', '시', '길', '바랄', '게요']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 여자 심리\n",
      "Predicted translation: 한 대화 나누 고 만날 필요 만날 수 있 어 플 아요 .\n",
      "Source Sentence:  여자 심리\n",
      "Model Prediction:  한 대화 나누 고 만날 필요 만날 수 있 어 플 아요 .\n",
      "Real:  ['든', '힘들', '어', '할', '거', '예요', '.']\n",
      "Score: 0.007174\n",
      "\n",
      "Input: 자고 말 했 어 마음 이 찢어지 네\n",
      "Predicted translation: 한 사람 이 예쁘 게 사람 좋 아 보이 네요 .\n",
      "Source Sentence:  자고 말 했 어 마음 이 찢어지 네\n",
      "Model Prediction:  한 사람 이 예쁘 게 사람 좋 아 보이 네요 .\n",
      "Real:  ['한', '사람', '도', '힘들', '죠', '.']\n",
      "Score: 0.008641\n",
      "\n",
      "Input: 아 줘 놀 으아 줘\n",
      "Predicted translation: 나가 면 놀 아요 .\n",
      "Source Sentence:  아 줘 놀 으아 줘\n",
      "Model Prediction:  나가 면 놀 아요 .\n",
      "Real:  ['놀', '아요', '.']\n",
      "Score: 0.022417\n",
      "\n",
      "Input: 집 알바 집 이게 뭐뭐 냐\n",
      "Predicted translation: 고 있 었 나 봐요 .\n",
      "Source Sentence:  집 알바 집 이게 뭐뭐 냐\n",
      "Model Prediction:  고 있 었 나 봐요 .\n",
      "Real:  ['상관없', '어요', '.', '칭찬', '해', '주', '고', '싶', '네요', '.']\n",
      "Score: 0.020256\n",
      "\n",
      "Input: 가 되 고\n",
      "Predicted translation: 이 시 고 싶 은 마음 에 큰 도움 이 될 거 예요 .\n",
      "Source Sentence:  가 되 고\n",
      "Model Prediction:  이 시 고 싶 은 마음 에 큰 도움 이 될 거 예요 .\n",
      "Real:  ['이', '많', '아', '지나', '봐요', '또한']\n",
      "Score: 0.006244\n",
      "\n",
      "Input: 광고 가 너무 길 어\n",
      "Predicted translation: 을 돌려 보 세요 .\n",
      "Source Sentence:  광고 가 너무 길 어\n",
      "Model Prediction:  을 돌려 보 세요 .\n",
      "Real:  ['을', '돌려', '보', '세요', '.']\n",
      "Score: 0.024808\n",
      "\n",
      "Input: 입 어 보 는 거 재밌 어 !\n",
      "Predicted translation: 이 요 !\n",
      "Source Sentence:  입 어 보 는 거 재밌 어 !\n",
      "Model Prediction:  이 요 !\n",
      "Real:  ['벌', '입', '어', '보', '고', '마음', '가', '드', '는', '걸로', '선택', '하', '세요', '.']\n",
      "Score: 0.000000\n",
      "\n",
      "Input: 알바 면접 간다\n",
      "Predicted translation: 도 해 보 면서 놀 은 알바 예요 .\n",
      "Source Sentence:  알바 면접 간다\n",
      "Model Prediction:  도 해 보 면서 놀 은 알바 예요 .\n",
      "Real:  ['도', '해', '보', '고', '싶', '은', '알바', '예요', '.']\n",
      "Score: 0.014400\n",
      "\n",
      "Input: 탈 때는 무슨 말 하 지\n",
      "Predicted translation: 은 생각 이 에요 .\n",
      "Source Sentence:  탈 때는 무슨 말 하 지\n",
      "Model Prediction:  은 생각 이 에요 .\n",
      "Real:  ['상영', '하', '는', '영화', '나', '맛집', '부터', '시작', '해', '보', '는', '것', '도', '좋', '을', '듯', '합니다', '.']\n",
      "Score: 0.009976\n",
      "\n",
      "Input: 가 오디션 붙 았 대\n",
      "Predicted translation: 끝 겠 어요 .\n",
      "Source Sentence:  가 오디션 붙 았 대\n",
      "Model Prediction:  끝 겠 어요 .\n",
      "Real:  ['해', '주', '세요', '.']\n",
      "Score: 0.027776\n",
      "\n",
      "Num of Sample: 68\n",
      "Total Score: 0.015971131389754555\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::5], dec_val[::5], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-monster",
   "metadata": {},
   "source": [
    "# 성과평가 및 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-secret",
   "metadata": {},
   "source": [
    "## (1) 성과평가\n",
    "- 챗봇 훈련데이터를 위한 전처리와 데이터증식으로 원데이터 11,823개에서 증식데이터 21,797개를 늘렸다.\n",
    "- 과제에서 제시된 하이퍼파라미터를 적용하여 에폭은 10회로 적용하고, 에폭마다 소요 시간은 1분 30초이고 Loss는 5.6231에서 시작해 0.4229로 줄어들었다.\n",
    "- 68문장에 대해 BLEU평가한 결과 0.015971131389754555로 매우 낮다.\n",
    "- 주어가 대부분 사라져 문장들이 불완전하다. 아마 위키의 단어장이어서 한국어의 특징을 제대로 반영하지 못한듯핟."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-frederick",
   "metadata": {},
   "source": [
    "## (2) 회고\n",
    "- 9장 내용에 BLEU라는 개념이 갑자기 나와, 이것 또한 뭐지하면서 위키백과를 찾아봤다. 자연어에서의 성과지표라는 것을 확인하고 따로 개념을 챙겼었다. 이번에 좀더 깊이 있는 내용을 접할 수 있었지만, 기본 개념만 챙겼다. 그런데 가만히 생각해보면 문장간에 얼마나 순서가 동일한 단어들이 배열되어 있냐는 건데....그래서인지 높은 점수가 나올 수는 없다. 의미상으로 체크할 수 있는 방법은? 이거 대단한 발견이겠는걸....\n",
    "- ko.bin을 로드 하는 데 실패를 반복하여 시간을 많이 소비되었다. 아쉬운 것은 결국 해결을 하지 못했다는 거다. 다급하게 위키의 한국 단어집을 하용하였다.\n",
    "- 아직까지 BoW를 만들고 벡터화하는 전체 프로세스를 정확히 파악하지 모하고 있다. 흐름도를 정립하여 다른 작업마다 혼돈이 없도록 해야겠다.\n",
    "- CV와 LNP의 근본적인 차이는 CV의 경우에는 원천데이타가 이미 벡터화되어 있다는 거다. 반면 NLP는 다양한 방법으로 벡터화할 수 있다. 앞단에서 이것을 어떻게 설정하느냐가 관건이다. 아직까지 각각의 차이와 성과와의 연결이 되지 않고 있다. 관련 동영상을 보면서 정립해야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
